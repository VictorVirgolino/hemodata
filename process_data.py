import pandas as pd
import numpy as np
import os
import re
import logging
from typing import List, Dict, Any, Set
import sys

# --- 1. Configura√ß√µes Globais ---

# Caminhos principais (ajuste se necess√°rio)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DADOS_BRUTOS_PATH = os.path.join(BASE_DIR, "dados_brutos")
PROCESSADOS_PATH = os.path.join(BASE_DIR, "dados_processados")
LOGS_PATH = os.path.join(PROCESSADOS_PATH, "logs")

# Dicion√°rios
DICIONARIO_PRINCIPAL_PATH = os.path.join(
    BASE_DIR, "dicionario_colunas_269_COM_TIPOS.xlsx"
)
DICIONARIO_MAPA_PATH = os.path.join(BASE_DIR, "dicionario_colunas_269_all.xlsx")

LOG_FILE_PATH = os.path.join(BASE_DIR, "processamento_log.txt")

# Constantes para deduplica√ß√£o
COLUNAS_CHAVE = [
    "cnpj",
    "ano_referencia",
    "periodo_referencia",
    "razao_social_nome_fantasia",
]
COLUNA_DATA = "data_envio"

# Mapeamento de abrevia√ß√µes para nomes
ESTADOS_MAPA = {
    "al": "Alagoas",
    "am": "Amazonas",
    "ap": "Amap√°",
    "ba": "Bahia",
    "ce": "Cear√°",
    "df": "Distrito Federal",
    "es": "Esp√≠rito Santo",
    "go": "Goi√°s",
    "hm": "Hemominas",
    "ma": "Maranh√£o",
    "mg": "Minas Gerais",
    "ms": "Mato Grosso do Sul",
    "mt": "Mato Grosso",
    "pa": "Par√°",
    "pb": "Para√≠ba",
    "pe": "Pernambuco",
    "pi": "Piau√≠",
    "pr": "Paran√°",
    "rj": "Rio de Janeiro",
    "rn": "Rio Grande do Norte",
    "ro": "Rond√¥nia",
    "rr": "Roraima",
    "rs": "Rio Grande do Sul",
    "sc": "Santa Catarina",
    "se": "Sergipe",
    "sp": "S√£o Paulo",
    "to": "Tocantins",
}

ESTADOS_PLANILHA = {
    "al": "HEMOPROD - ALAGOAS",
    "am": "HEMOPROD - AMAZONAS",
    "ap": "HEMOPROD - AMAPA",
    "ba": "HEMOPROD - BAHIA",
    "ce": "Planilha1",
    "df": "HEMOPROD - DISTRITOFEDERAL",
    "es": "HEMOPROD - ESPIRITOSANTO",
    "go": "HEMOPROD - GOIAS",
    "hm": "HEMOPROD - HEMOMINAS",
    "ma": "HEMOPROD - MARANHAO",
    "mg": "HEMOPROD - MINASGERAIS",
    "ms": "HEMOPROD - MATOGROSSODOSUL",
    "mt": "HEMOPROD - MATOGROSSO",
    "pa": "HEMOPROD - PARA",
    "pb": "HEMOPROD - PARAIBA",
    "pe": "HEMOPROD - PERNAMBUCO",
    "pi": "HEMOPROD - PIAUI",
    "pr": "HEMOPROD - PARANA",
    "rj": "Hemoprod_RJ",
    "rn": "HEMOPROD - RIOGRANDEDONORTE",
    "ro": "HEMOPROD - RONDONIA",
    "rr": "HEMOPROD - RORAIMA",
    "rs": "HEMOPROD - RIOGRANDEDOSUL",
    "sc": "HEMOPROD - SANTACATARINA",
    "se": "HEMOPROD - SERGIPE",
    "sp": "Hemoprod_SP",
    "to": "HEMOPROD - TOCANTINS",
}


# --- 2. Fun√ß√µes Auxiliares ---


def setup_logging_geral():
    """Configura o logging geral (arquivo principal)."""
    # Remove todos os handlers existentes
    root = logging.getLogger()
    for handler in root.handlers[:]:
        root.removeHandler(handler)

    root.setLevel(logging.INFO)

    # Formato
    fmt = "%(asctime)s - %(levelname)s - %(message)s"
    formatter = logging.Formatter(fmt)

    # Handler para arquivo geral
    fh = logging.FileHandler(LOG_FILE_PATH, mode="w", encoding="utf-8")
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)
    root.addHandler(fh)

    # Handler para console
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)
    root.addHandler(ch)


def criar_logger_arquivo(nome_arquivo: str):
    """
    Cria um logger isolado para um arquivo espec√≠fico.
    Retorna o logger e os handlers para fechar depois.
    """
    # Cria diret√≥rio de logs se n√£o existir
    os.makedirs(LOGS_PATH, exist_ok=True)

    # Nome do logger √∫nico
    logger_name = f"etl_arquivo_{nome_arquivo}"
    logger = logging.getLogger(logger_name)

    # Remove handlers anteriores (caso exista)
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    logger.setLevel(logging.INFO)
    logger.propagate = False  # N√ÉO propaga para o root logger

    # Formato com prefixo do arquivo
    fmt = f"%(asctime)s - [%(levelname)s] - [{nome_arquivo}] - %(message)s"
    formatter = logging.Formatter(fmt)

    # Handler para arquivo espec√≠fico
    log_file_path = os.path.join(LOGS_PATH, f'{nome_arquivo.replace(".xlsx", "")}.log')
    fh = logging.FileHandler(log_file_path, mode="w", encoding="utf-8")
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    # Handler para console
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    return logger, [fh, ch]


def fechar_logger(logger, handlers):
    """Fecha e remove todos os handlers de um logger."""
    for handler in handlers:
        try:
            handler.flush()
            handler.close()
            logger.removeHandler(handler)
        except Exception:
            pass


def normalizar_periodo_referencia(df: pd.DataFrame, logger) -> pd.DataFrame:
    """
    Normaliza o campo periodo_referencia quando vier com ano junto.
    Exemplos: 'Outubro/2022', 'Consolidado/2023', 'Consolidado 2022', '10/2022'
    Extrai o ano (4 d√≠gitos) para ano_referencia e deixa apenas o per√≠odo em periodo_referencia.
    """
    if 'periodo_referencia' not in df.columns:
        logger.warning("Coluna 'periodo_referencia' n√£o encontrada. Pulando normaliza√ß√£o.")
        return df
    
    # Cria a coluna ano_referencia se n√£o existir
    if 'ano_referencia' not in df.columns:
        logger.info("Coluna 'ano_referencia' n√£o existe. Criando...")
        df['ano_referencia'] = pd.NA
    
    registros_alterados = 0
    
    for idx in df.index:
        valor = df.at[idx, 'periodo_referencia']
        
        if pd.isna(valor):
            continue
            
        valor_str = str(valor).strip()
        
        # Busca por 4 d√≠gitos consecutivos (ano)
        ano_match = re.search(r'\b(\d{4})\b', valor_str)
        
        if ano_match:
            ano_encontrado = int(ano_match.group(1))
            
            # Remove o ano do texto do per√≠odo (e tamb√©m remove / ou espa√ßos extras)
            periodo_limpo = re.sub(r'\s*/?\s*\d{4}\b', '', valor_str).strip()
            periodo_limpo = re.sub(r'/\s*$', '', periodo_limpo).strip()  # Remove barra final se sobrar
            periodo_limpo = re.sub(r'^\s*/', '', periodo_limpo).strip()  # Remove barra inicial se sobrar
            
            # Atualiza periodo_referencia (sem o ano)
            if periodo_limpo:
                df.at[idx, 'periodo_referencia'] = periodo_limpo
            
            # Atualiza ano_referencia
            ano_atual = df.at[idx, 'ano_referencia']
            
            # Verifica se est√° vazio (NaN, None, string vazia, etc.)
            ano_vazio = pd.isna(ano_atual) or ano_atual == '' or ano_atual is None
            
            if ano_vazio:
                df.at[idx, 'ano_referencia'] = ano_encontrado
                registros_alterados += 1
            else:
                # Se j√° existe um ano, verifica se √© diferente
                try:
                    ano_atual_int = int(ano_atual)
                    if ano_atual_int != ano_encontrado:
                        logger.warning(f"Conflito de ano na linha {idx}: ano_referencia={ano_atual_int}, encontrado no per√≠odo={ano_encontrado}. Mantendo {ano_encontrado}.")
                        df.at[idx, 'ano_referencia'] = ano_encontrado
                        registros_alterados += 1
                except (ValueError, TypeError):
                    # Se n√£o conseguir converter, sobrescreve
                    df.at[idx, 'ano_referencia'] = ano_encontrado
                    registros_alterados += 1
    
    if registros_alterados > 0:
        logger.info(f"‚úì Normalizados {registros_alterados} registros com ano no per√≠odo")
    else:
        logger.info("‚úì Nenhum registro necessitou normaliza√ß√£o de per√≠odo/ano")
    
    return df

def normalizar_municipio_estado(df: pd.DataFrame, logger) -> pd.DataFrame:  
    """  
    Normaliza o campo municipio quando vier com estado junto.  
    Exemplos: 'Macei√≥, Alagoas', 'S√£o Paulo, SP', 'Rio de Janeiro,RJ'  
    Extrai o estado para a coluna 'estado' e deixa apenas o munic√≠pio em 'municipio'.  
    """  
    if 'municipio' not in df.columns:  
        logger.warning("Coluna 'municipio' n√£o encontrada. Pulando normaliza√ß√£o.")  
        return df  
      
    # Cria a coluna estado se n√£o existir  
    if 'estado' not in df.columns:  
        logger.info("Coluna 'estado' n√£o existe. Criando...")  
        df['estado'] = ""  
      
    registros_alterados = 0  
      
    for idx in df.index:  
        valor = df.at[idx, 'municipio']  
          
        if pd.isna(valor) or valor == '':  
            continue  
              
        valor_str = str(valor).strip()  
          
        # Verifica se tem v√≠rgula (separador comum entre munic√≠pio e estado)  
        if ',' in valor_str:  
            partes = valor_str.split(',')  
              
            if len(partes) >= 2:  
                municipio_limpo = partes[0].strip()  
                estado_extraido = partes[1].strip()  
                  
                # Atualiza municipio (sem o estado)  
                df.at[idx, 'municipio'] = municipio_limpo  
                  
                # Atualiza estado  
                estado_atual = df.at[idx, 'estado']  
                  
                # Verifica se est√° vazio  
                estado_vazio = pd.isna(estado_atual) or estado_atual == '' or estado_atual is None  
                  
                if estado_vazio:  
                    df.at[idx, 'estado'] = estado_extraido  
                    registros_alterados += 1  
                else:  
                    # Se j√° existe um estado, verifica se √© diferente  
                    if str(estado_atual).strip() != estado_extraido:  
                        logger.warning(f"Conflito de estado na linha {idx}: estado={estado_atual}, encontrado no munic√≠pio={estado_extraido}. Mantendo {estado_extraido}.")  
                        df.at[idx, 'estado'] = estado_extraido  
                        registros_alterados += 1  
      
    if registros_alterados > 0:  
        logger.info(f"‚úì Normalizados {registros_alterados} registros com estado no munic√≠pio")  
    else:  
        logger.info("‚úì Nenhum registro necessitou normaliza√ß√£o de munic√≠pio/estado")  
      
    return df


def gerar_lista_arquivos() -> List[Dict[str, str]]:
    """Gera o dicion√°rio de lista com os arquivos e planilhas a processar."""
    arquivos_para_processar = []

    for abrev, estado in ESTADOS_MAPA.items():
        # Define o nome do ARQUIVO
        if abrev == "hm":
            arquivo_bruto = "Hemoprod_Hemominas.xlsx"
        else:
            arquivo_bruto = f"Hemoprod_{abrev.upper()}.xlsx"

        # Busca o nome da PLANILHA
        planilha = ESTADOS_PLANILHA.get(abrev)

        if not planilha:
            logging.warning(f"Nome da planilha n√£o encontrado para '{abrev}'. Pulando.")
            continue

        # Define o nome do arquivo de SA√çDA
        arquivo_processado = f"hemoprod_{abrev.lower()}.xlsx"

        arquivos_para_processar.append(
            {
                "arquivo_bruto": arquivo_bruto,
                "planilha": planilha,
                "arquivo_processado": arquivo_processado,
                "sigla": abrev.upper(),
            }
        )

    return arquivos_para_processar


def carregar_dicionario(caminho_dicionario: str) -> pd.DataFrame:
    """Carrega e limpa o dicion√°rio de colunas."""
    logging.info(f"Carregando dicion√°rio de {caminho_dicionario}...")
    dicionario = pd.read_excel(caminho_dicionario)

    # Limpeza dos nomes originais no dicion√°rio
    if "nome_original" in dicionario.columns:
        dicionario["nome_original"] = (
            dicionario["nome_original"].astype(str).str.strip().str.replace("\xa0", " ")
        )
    else:
        raise KeyError("Coluna 'nome_original' n√£o encontrada no dicion√°rio.")

    if "nome_sql" not in dicionario.columns:
        raise KeyError("Coluna 'nome_sql' n√£o encontrada no dicion√°rio.")

    # Carrega e limpa a coluna de tipos
    if "tipo_dados" in dicionario.columns:
        dicionario["tipo_dados"] = (
            dicionario["tipo_dados"].astype(str).str.strip().str.lower()
        )
        dicionario["tipo_dados"] = dicionario["tipo_dados"].replace("nan", "object")
        dicionario["tipo_dados"] = dicionario["tipo_dados"].fillna("object")
    else:
        if "all.xlsx" not in caminho_dicionario:
            raise KeyError(
                "Coluna 'tipo_dados' n√£o encontrada no dicion√°rio principal."
            )
        dicionario["tipo_dados"] = "object"

    logging.info(
        f"Dicion√°rio {os.path.basename(caminho_dicionario)} carregado com sucesso."
    )
    return dicionario


def processar_arquivo(
    info_arquivo: Dict[str, str],
    df_dicionario: pd.DataFrame,
    mapa_renomeacao: Dict[str, str],
    colunas_sql_desejadas: Set[str],
):
    """Executa o pipeline completo de processamento para um √∫nico arquivo."""

    arquivo_dados_path = os.path.join(DADOS_BRUTOS_PATH, info_arquivo["arquivo_bruto"])
    nome_planilha = info_arquivo["planilha"]
    nome_saida = info_arquivo["arquivo_processado"]
    sigla = info_arquivo["sigla"]

    # Cria logger isolado para este arquivo
    logger, handlers = criar_logger_arquivo(nome_saida)

    try:
        logger.info("=" * 60)
        logger.info(f"INICIANDO PROCESSAMENTO: {sigla}")
        logger.info("=" * 60)

        # --- 1. Carregar Dados ---
        if not os.path.exists(arquivo_dados_path):
            logger.warning(f"Arquivo n√£o encontrado: {arquivo_dados_path}")
            logger.warning("PULANDO ARQUIVO")
            return

        logger.info(f"Carregando arquivo: {info_arquivo['arquivo_bruto']}")
        logger.info(f"Planilha: {nome_planilha}")

        try:
            df = pd.read_excel(arquivo_dados_path, sheet_name=nome_planilha)
        except Exception as e:
            logger.error(f"ERRO ao carregar arquivo: {e}")
            logger.error("PULANDO ARQUIVO")
            return

        # M√©tricas iniciais
        col_count_original = len(df.columns)
        row_count_original = len(df)
        logger.info(f"‚úì Colunas Originais: {col_count_original}")
        logger.info(f"‚úì Linhas Originais: {row_count_original}")

        # --- 2. Limpeza e Renomea√ß√£o ---
        logger.info("Iniciando limpeza e renomea√ß√£o de colunas...")
        print(f"   üîß Limpando e renomeando colunas...")
        sys.stdout.flush()

        mapa_limpeza = {
            col: str(col).strip().replace("\xa0", " ") for col in df.columns
        }
        df.rename(columns=mapa_limpeza, inplace=True)
        df.rename(columns=mapa_renomeacao, inplace=True)

        # # ===== DEBUG TEMPOR√ÅRIO - REMOVER DEPOIS =====
        # if sigla == 'DF':
        #     logger.info("üîç DEBUG - Colunas problem√°ticas do DF:")
        #     for col in df.columns:
        #         if 'Raz√£o Social' in str(col) or 'razao_social' in str(col).lower():
        #             logger.info(f"  Coluna encontrada: '{col}'")
        #             logger.info(f"  Tamanho: {len(str(col))} caracteres")
        #             logger.info(f"  Repr: {repr(col)}")
        #             logger.info(f"  Est√° no mapa? {col in mapa_renomeacao}")
        #             if col in mapa_renomeacao:
        #                 logger.info(f"  Mapeia para: {mapa_renomeacao[col]}")
        #             logger.info("")

        #     logger.info("üîç DEBUG - Verificando dicion√°rio de mapeamento:")
        #     for key in mapa_renomeacao.keys():
        #         if 'Raz√£o Social' in str(key):
        #             logger.info(f"  Chave no dicion√°rio: '{key}'")
        #             logger.info(f"  Tamanho: {len(str(key))} caracteres")
        #             logger.info(f"  Repr: {repr(key)}")
        #             logger.info(f"  Mapeia para: {mapa_renomeacao[key]}")
        #             logger.info("")
        # # ===== FIM DO DEBUG =====

        # # ===== DEBUG TEMPOR√ÅRIO - MUNIC√çPIO =====  
        # logger.info("üîç DEBUG - Investigando campo Munic√≠pio:")  
        # logger.info(f"Total de colunas no DataFrame: {len(df.columns)}")  
          
        # # Procura por qualquer coluna que contenha "Munic√≠pio" ou "municipio"  
        # for col in df.columns:  
        #     col_lower = str(col).lower()  
        #     if 'munic√≠pio' in col_lower or 'municipio' in col_lower:  
        #         logger.info(f"\n  ‚úì Coluna encontrada: '{col}'")  
        #         logger.info(f"    Tamanho: {len(str(col))} caracteres")  
        #         logger.info(f"    Repr: {repr(col)}")  
        #         logger.info(f"    Bytes: {str(col).encode('utf-8')}")  
        #         logger.info(f"    Est√° no mapa? {col in mapa_renomeacao}")  
        #         if col in mapa_renomeacao:  
        #             logger.info(f"    Mapeia para: {mapa_renomeacao[col]}")  
        #         else:  
        #             logger.info(f"    ‚ùå N√ÉO est√° no mapa de renomea√ß√£o!")  
                  
        #         # Mostra amostra dos dados  
        #         valores_nao_nulos = df[col].dropna().head(3)  
        #         if len(valores_nao_nulos) > 0:  
        #             logger.info(f"    Amostra de dados:")  
        #             for val in valores_nao_nulos:  
        #                 logger.info(f"      - {val}")  
  
        # logger.info("\nüîç DEBUG - Verificando dicion√°rio de mapeamento:")  
        # logger.info(f"Total de chaves no mapa: {len(mapa_renomeacao)}")  
          
        # # Procura por chaves que contenham "Munic√≠pio" ou "municipio"  
        # encontrou_no_mapa = False  
        # for key in mapa_renomeacao.keys():  
        #     key_lower = str(key).lower()  
        #     if 'munic√≠pio' in key_lower or 'municipio' in key_lower:  
        #         encontrou_no_mapa = True  
        #         logger.info(f"\n  ‚úì Chave no dicion√°rio: '{key}'")  
        #         logger.info(f"    Tamanho: {len(str(key))} caracteres")  
        #         logger.info(f"    Repr: {repr(key)}")  
        #         logger.info(f"    Bytes: {str(key).encode('utf-8')}")  
        #         logger.info(f"    Mapeia para: {mapa_renomeacao[key]}")  
          
        # if not encontrou_no_mapa:  
        #     logger.warning("  ‚ùå Nenhuma chave com 'munic√≠pio' encontrada no mapa!")  
          
        # logger.info("\n" + "="*60)  
        # # ===== FIM DO DEBUG MUNIC√çPIO =====

        colunas_apos_renomeacao = set(df.columns)
        logger.info("‚úì Colunas limpas e renomeadas")

        # --- 2.5. Normaliza√ß√£o de Per√≠odo/Ano (ANTES da aplica√ß√£o de tipos) ---
        logger.info("Normalizando per√≠odo e ano de refer√™ncia...")
        print(f"   üìÖ Normalizando per√≠odo/ano...")
        sys.stdout.flush()
        df = normalizar_periodo_referencia(df, logger)
        logger.info("‚úì Per√≠odo/Ano normalizado")

        # --- 2.6. Normaliza√ß√£o de Munic√≠pio/Estado ---  
        logger.info("Normalizando munic√≠pio e estado...")  
        print(f"   üó∫Ô∏è  Normalizando munic√≠pio/estado...")  
        sys.stdout.flush()  
        df = normalizar_municipio_estado(df, logger)  
        logger.info("‚úì Munic√≠pio/Estado normalizado")
        
        # --- 3. Aplica√ß√£o de Tipos ---
        logger.info("Aplicando tipos de dados...")

        mapa_tipos = pd.Series(
            df_dicionario["tipo_dados"].values, index=df_dicionario["nome_sql"]
        ).to_dict()

        for col in df.columns:
            if col not in colunas_sql_desejadas:
                continue

            tipo_desejado = mapa_tipos.get(col, "object")

            try:
                if tipo_desejado in ("int", "integer", "int64"):
                    if df[col].dtype == "object":
                        df[col] = (
                            df[col].astype(str).str.replace(r"[.,]", "", regex=True)
                        )
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                    df[col] = df[col].round(0).astype("Int64")

                elif tipo_desejado in ("float", "decimal", "float64"):
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                    df[col] = df[col].astype("float64")

                elif tipo_desejado in ("datetime", "date", "datetime64[ns]"):
                    df[col] = pd.to_datetime(df[col], errors="coerce")

                elif tipo_desejado in ("string", "text", "object"):
                    df[col] = df[col].astype("object")

            except Exception as e:
                logger.warning(f"Falha ao converter '{col}' para {tipo_desejado}: {e}")
                try:
                    df[col] = df[col].astype("object")
                except Exception as e_obj:
                    logger.error(
                        f"ERRO CR√çTICO ao for√ßar '{col}' para 'object': {e_obj}"
                    )

        logger.info("‚úì Tipos aplicados")

        # --- 4. An√°lise de Schema ---
        colunas_faltantes = list(colunas_sql_desejadas - colunas_apos_renomeacao)
        colunas_a_mais = list(colunas_apos_renomeacao - colunas_sql_desejadas)

        logger.info(f"‚úì Colunas Faltantes: {len(colunas_faltantes)}")
        if colunas_faltantes:
            logger.info(
                f"  ‚Üí {colunas_faltantes[:5]}{'...' if len(colunas_faltantes) > 5 else ''}"
            )

        logger.info(f"‚úì Colunas A Mais: {len(colunas_a_mais)}")
        if colunas_a_mais:
            logger.info(
                f"  ‚Üí {colunas_a_mais[:5]}{'...' if len(colunas_a_mais) > 5 else ''}"
            )

      # --- 5. Padroniza√ß√£o do Schema ---
        logger.info("Padronizando schema...")

        # Remove colunas a mais
        if colunas_a_mais:
            colunas_para_dropar = [col for col in colunas_a_mais if col in df.columns]
            df.drop(columns=colunas_para_dropar, inplace=True)

        # Adiciona colunas faltantes
        if colunas_faltantes:
            dicionario_faltante = df_dicionario[
                df_dicionario["nome_sql"].isin(colunas_faltantes)
            ]

            for col_sql in colunas_faltantes:
                # IMPORTANTE: S√≥ cria se a coluna N√ÉO existir
                if col_sql in df.columns:
                    logger.info(f"Coluna '{col_sql}' j√° existe, mantendo valores atuais")
                    continue
                
                tipo_desejado_row = dicionario_faltante[
                    dicionario_faltante["nome_sql"] == col_sql
                ]

                if not tipo_desejado_row.empty:
                    tipo_desejado = tipo_desejado_row["tipo_dados"].iloc[0]

                    try:
                        if tipo_desejado in ("int", "integer", "int64"):
                            df[col_sql] = pd.Series(0, index=df.index, dtype="Int64")
                        elif tipo_desejado in ("float", "decimal", "float64"):
                            df[col_sql] = pd.Series(0.0, index=df.index, dtype="float64")
                        elif tipo_desejado in ("datetime", "date", "datetime64[ns]"):
                            df[col_sql] = pd.Series(pd.NaT, index=df.index, dtype="datetime64[ns]")
                        else:
                            df[col_sql] = pd.Series("", index=df.index, dtype="object")
                    except Exception as e:
                        logger.warning(
                            f"Falha ao criar coluna '{col_sql}' com tipo {tipo_desejado}: {e}"
                        )
                        df[col_sql] = pd.Series("", index=df.index, dtype="object")
                else:
                    # Se n√£o encontrar no dicion√°rio, cria como string vazia
                    df[col_sql] = pd.Series("", index=df.index, dtype="object")

        # Reordena as colunas
        colunas_finais_ordenadas = [
            col for col in df_dicionario["nome_sql"] if col in df.columns
        ]
        df = df[colunas_finais_ordenadas]

        logger.info(f"‚úì Schema padronizado - Colunas Finais: {len(df.columns)}")

        # --- 5.5. Preenchimento de valores nulos em colunas Int64 ---
        logger.info("Preenchendo valores nulos em colunas Int64 com 0...")
        print(f"   üî¢ Preenchendo valores nulos em colunas Int64...")
        sys.stdout.flush()
        
        colunas_int64_preenchidas = 0
        for col in df.columns:
            if pd.api.types.is_integer_dtype(df[col]):
                valores_nulos = df[col].isna().sum()
                if valores_nulos > 0:
                    df[col] = df[col].fillna(0)
                    colunas_int64_preenchidas += 1
                    logger.info(f"  ‚Üí Coluna '{col}': {valores_nulos} valores nulos preenchidos com 0")
        
        if colunas_int64_preenchidas > 0:
            logger.info(f"‚úì {colunas_int64_preenchidas} colunas Int64 preenchidas com 0")
        else:
            logger.info("‚úì Nenhuma coluna Int64 necessitou preenchimento")

        # --- 6. Deduplica√ß√£o ---
        logger.info("Iniciando deduplica√ß√£o...")

        colunas_necessarias_dedup = COLUNAS_CHAVE + [COLUNA_DATA]

        if not all(col in df.columns for col in colunas_necessarias_dedup):
            colunas_faltantes_dedup = [
                col for col in colunas_necessarias_dedup if col not in df.columns
            ]
            logger.error(
                f"Colunas necess√°rias para deduplica√ß√£o n√£o encontradas: {colunas_faltantes_dedup}"
            )
            logger.warning("Pulando deduplica√ß√£o")
            df_deduplicado = df
        else:
            if not pd.api.types.is_datetime64_any_dtype(df[COLUNA_DATA]):
                logger.info(f"Convertendo '{COLUNA_DATA}' para datetime...")
                df[COLUNA_DATA] = pd.to_datetime(df[COLUNA_DATA], errors="coerce")

            df_ordenado = df.sort_values(by=COLUNA_DATA, ascending=True)
            mascara_duplicatas = df_ordenado.duplicated(
                subset=COLUNAS_CHAVE, keep="last"
            )
            df_deduplicado = df_ordenado[~mascara_duplicatas]

            duplicatas_removidas = len(df) - len(df_deduplicado)
            logger.info(
                f"‚úì Deduplica√ß√£o conclu√≠da - Removidas {duplicatas_removidas} linhas duplicadas"
            )

        row_count_final = len(df_deduplicado)
        logger.info(f"‚úì Linhas Finais: {row_count_final}")

        # --- 7. Salvar ---
        logger.info("Salvando arquivo processado...")

        output_path = os.path.join(PROCESSADOS_PATH, nome_saida)
        df_deduplicado.to_excel(output_path, index=False)

        logger.info("=" * 60)
        logger.info(f"‚úÖ PROCESSAMENTO CONCLU√çDO COM SUCESSO")
        logger.info(f"‚úÖ Arquivo salvo em: {output_path}")
        logger.info("=" * 60)
        logger.info("")  # Linha em branco para separa√ß√£o

    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"‚ùå FALHA NO PROCESSAMENTO: {e}")
        logger.error("=" * 60)
        logger.error("", exc_info=True)

    finally:
        # Fecha e limpa o logger deste arquivo
        fechar_logger(logger, handlers)
        # For√ßa flush do stdout
        sys.stdout.flush()


# --- 3. Execu√ß√£o Principal ---


def main():
    """Orquestra o processo de ETL."""

    # Setup do logging geral
    setup_logging_geral()

    logging.info("")
    logging.info("#" * 80)
    logging.info("### IN√çCIO DO SCRIPT DE PROCESSAMENTO ETL ###")
    logging.info("#" * 80)
    logging.info("")

    # Garante que os diret√≥rios existam
    os.makedirs(PROCESSADOS_PATH, exist_ok=True)
    os.makedirs(LOGS_PATH, exist_ok=True)

    # Gera a lista de arquivos
    arquivos_para_processar = gerar_lista_arquivos()
    if not arquivos_para_processar:
        logging.warning("Nenhum arquivo configurado para processamento.")
        return

    logging.info(f"Total de arquivos a processar: {len(arquivos_para_processar)}")
    logging.info("")

    # Carrega os Dicion√°rios
    try:
        dicionario_principal = carregar_dicionario(DICIONARIO_PRINCIPAL_PATH)
        df_dicionario = dicionario_principal
        colunas_sql_desejadas = set(df_dicionario["nome_sql"].tolist())
        logging.info(f"‚úì Dicion√°rio Principal: {len(colunas_sql_desejadas)} colunas")

        dicionario_mapa = carregar_dicionario(DICIONARIO_MAPA_PATH)
        mapa_renomeacao = pd.Series(
            dicionario_mapa["nome_sql"].values, index=dicionario_mapa["nome_original"]
        ).to_dict()
        logging.info(f"‚úì Dicion√°rio de Mapeamento: {len(mapa_renomeacao)} mapeamentos")
        logging.info("")

    except Exception as e:
        logging.critical(f"‚ùå Falha ao carregar os dicion√°rios: {e}")
        logging.critical("ABORTANDO SCRIPT")
        return

    # Loop de Processamento - UM ARQUIVO POR VEZ
    for idx, info_arquivo in enumerate(arquivos_para_processar, 1):
        logging.info(f"\n{'='*80}")
        logging.info(
            f"Processando arquivo {idx}/{len(arquivos_para_processar)}: {info_arquivo['arquivo_bruto']}"
        )
        logging.info(f"{'='*80}\n")

        # Processa o arquivo (com logger isolado)
        processar_arquivo(
            info_arquivo, df_dicionario, mapa_renomeacao, colunas_sql_desejadas
        )

        # For√ßa flush entre arquivos
        sys.stdout.flush()

        # Pequena pausa para garantir que tudo foi escrito
        import time

        time.sleep(0.1)

    logging.info("")
    logging.info("#" * 80)
    logging.info("### PROCESSAMENTO CONCLU√çDO ###")
    logging.info("#" * 80)
    logging.info("")


if __name__ == "__main__":
    main()

